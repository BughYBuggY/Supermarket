{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0daa18fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import speech_recognition as sr    # SESİ METİNE ÇEVİRMEK İÇİN\n",
    "import numpy as np \n",
    "import pandas as pd                # DATA SETİNİ OKUMA VE DÜZENLEME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c79aea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN MODELİNİ KURMAK İÇİN KERAS KUTUPHANESİNİ KULLANACAĞIZ\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1291e2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dosyayı utf-16 kodlaması ile oku\n",
    "dataset = pd.read_csv(\"C:/Users/bgurd/magaza_yorumlari_duygu_analizi.csv\", encoding='utf-16')\n",
    "\n",
    "etiket = dataset['Durum'].values.tolist()  \n",
    "data = dataset['Görüş'].values.tolist()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ee71eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tüm verileri string'e dönüştür\n",
    "data = [str(review) for review in data]\n",
    "\n",
    "# Veriyi eğitim ve test setlerine ayırın\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, etiket, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09f90cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tüm verileri string'e dönüştür\n",
    "data = [str(review) for review in data]\n",
    "\n",
    "# Tokenizer kullanarak metinleri tokenize et\n",
    "num_words = 10000\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "902c9db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tüm verileri string'e dönüştür (emin olmak için)\n",
    "x_train = [str(review) for review in x_train]\n",
    "x_test = [str(review) for review in x_test]\n",
    "\n",
    "# Tokenleştirme\n",
    "x_train_tokens = tokenizer.texts_to_sequences(x_train)  # Eğitim setini tokenleştirdik\n",
    "x_test_tokens = tokenizer.texts_to_sequences(x_test)    # Test setini tokenleştirdik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb7ff802",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26, 1060, 14, 3643, 1819, 753, 4304], [93, 2, 120, 3305, 6886, 584, 4147, 3306, 14, 14], [4, 479, 11, 5760, 13, 24], [16, 152, 173, 24], [16, 1, 4938, 66, 47, 703, 2, 42, 1, 4, 33, 2695, 4034, 80, 2, 7, 641, 172, 6335, 13, 24, 218, 13, 396, 131, 3, 108]]\n",
      "[[2041, 4275, 2780, 605, 6293, 289, 588, 449, 2, 358, 1, 257, 20, 3253, 2886, 6633, 5589, 145, 320, 2093, 1, 39, 76, 69, 1326, 78, 114, 2164, 119, 3702, 1352, 5, 213, 13, 102], [2, 152, 8273, 504, 212, 783, 10, 57, 608, 25, 2, 35, 90, 38, 45, 123, 557, 52], [1, 2189, 2676, 104, 2, 7, 143, 2, 59], [275, 7459, 356, 912, 1238, 1007, 727, 381, 190, 79, 705, 4398, 2996, 20, 11, 202, 4945, 1870, 4805, 12], [35, 655, 3, 2200, 2660, 385, 4, 21, 7, 1, 4]]\n"
     ]
    }
   ],
   "source": [
    "# Kontrol amaçlı ilk birkaç tokenleştirilmiş metni yazdır\n",
    "print(x_train_tokens[:5])\n",
    "print(x_test_tokens[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee371bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RNN aynı boyutta veri kümelerinde çalıştığından datalar kısaltcaz veya 0 atarak uzatcaz (eşitlemek için)\n",
    "num_tokens = [len(tokens) for tokens in x_train_tokens + x_test_tokens ]   ## her yorumda kac token oldugu hesaplanır\n",
    "num_tokens = np.array(num_tokens) ## dizi numpy arraye ceviriyoruz\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens) ## cumlelerin ortalama kelime sayısının 2 fazlasıyla standart sapmasını topluyoruz\n",
    "max_tokens = int(max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9437baf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pading eklenmiş verileri tanımlayın\n",
    "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens) # pad_sequences keras kutupanesının fonksıyonudur\n",
    "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens)\n",
    "\n",
    "# Daha sonra yeniden şekillendirme işlemini gerçekleştirin\n",
    "x_train_pad_reshaped = x_train_pad.reshape(x_train_pad.shape[0], -1)\n",
    "x_test_pad_reshaped = x_test_pad.reshape(x_test_pad.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56365a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() ## ardışık model oluşturma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab40dd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 35 ## kullanılcak kelime vektörlerinin uzunluğu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "779f9435",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bgurd\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# input içindeki kelimelerin vektörlerini output olarak verir\n",
    "model.add(Embedding(input_dim=num_words,\n",
    "                    output_dim=embedding_size,\n",
    "                    input_length=max_tokens,\n",
    "                    name='layer'))          ## 9000 e 50 boyutunda matris oluşturuyoruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "293ae586",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.add(GRU(units= 16, return_sequences=True)) # unit = 16 00 nöron sayısı // return_sequence bir sonraki layerları ekleyecegımız ıcın true\n",
    "model.add(GRU(units= 8, return_sequences=True))\n",
    "model.add(GRU(units= 4)) # son noron oldugu ıcın return_sequeces oto false bırakıyoruz\n",
    "model.add(Dense(1, activation='sigmoid')) # cıkıs noronu oldugu ıcın Dense kullanıldı ve sigmoid fonk 0 ile 1 arası degerler doner yanı mutlu ve mutsuz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dc3b656",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optimizasyon\n",
    "optimizer = Adam(learning_rate=1e-3)  # 1e-3 = 0.001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e8f2a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelin Derlenmesi \n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy']) # yanlızca 2 sınıf oldugu ıcın loss için binary_crossentropy kullandık // metris masarı oranını gormek ıcın kullanıldı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "445ae1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tarafsız', 'Tarafsız', 'Olumlu', 'Olumlu', 'Olumlu', 'Olumlu', 'Olumsuz', 'Olumlu', 'Olumlu', 'Tarafsız']\n",
      "['Olumsuz', 'Tarafsız', 'Olumlu', 'Olumsuz', 'Olumlu', 'Olumlu', 'Olumsuz', 'Olumsuz', 'Tarafsız', 'Olumlu']\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:10])  # İlk 10 etiketi yazdır\n",
    "print(y_test[:10])   # Test setindeki ilk 10 etiketi yazdır"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a16037e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_index = {\"Olumlu\": 1, \"Olumsuz\": 0, \"Tarafsız\": 2}\n",
    "\n",
    "y_train_index = np.array([label_to_index[label] for label in y_train])\n",
    "y_test_index = np.array([label_to_index[label] for label in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "601524a8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.3785 - loss: 0.5410\n",
      "Epoch 2/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 68ms/step - accuracy: 0.3746 - loss: 0.3362\n",
      "Epoch 3/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 62ms/step - accuracy: 0.5281 - loss: 0.0673\n",
      "Epoch 4/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 61ms/step - accuracy: 0.6091 - loss: -0.2356\n",
      "Epoch 5/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 60ms/step - accuracy: 0.6577 - loss: -0.4147\n",
      "Epoch 6/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 63ms/step - accuracy: 0.6680 - loss: -0.5904\n",
      "Epoch 7/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 62ms/step - accuracy: 0.6893 - loss: -0.6640\n",
      "Epoch 8/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 63ms/step - accuracy: 0.6940 - loss: -0.8486\n",
      "Epoch 9/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 63ms/step - accuracy: 0.6930 - loss: -0.9334\n",
      "Epoch 10/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 65ms/step - accuracy: 0.6990 - loss: -1.0554\n",
      "Epoch 11/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 64ms/step - accuracy: 0.6990 - loss: -1.1819\n",
      "Epoch 12/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 65ms/step - accuracy: 0.7055 - loss: -1.2155\n",
      "Epoch 13/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 63ms/step - accuracy: 0.7036 - loss: -1.3287\n",
      "Epoch 14/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 60ms/step - accuracy: 0.7069 - loss: -1.4004\n",
      "Epoch 15/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 60ms/step - accuracy: 0.7029 - loss: -1.5172\n",
      "Epoch 16/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 62ms/step - accuracy: 0.7123 - loss: -1.5849\n",
      "Epoch 17/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 64ms/step - accuracy: 0.7070 - loss: -1.7107\n",
      "Epoch 18/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 60ms/step - accuracy: 0.7064 - loss: -1.8632\n",
      "Epoch 19/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 60ms/step - accuracy: 0.7033 - loss: -1.9035\n",
      "Epoch 20/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 64ms/step - accuracy: 0.7044 - loss: -2.0064\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1a32a727f90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## MODELİN EĞİTİMİ\n",
    "model.fit(x_train_pad, y_train_index, epochs=20, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "184aafcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speak : \n",
      "soylediginiz : model düzgün çalışmıyor\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "[[0.02029321]]\n",
      "Negatif : 🙁\n"
     ]
    }
   ],
   "source": [
    "r = sr.Recognizer()\n",
    "import emoji\n",
    "facesmiling='\\U0001F600'\n",
    "\n",
    "with sr.Microphone() as source:\n",
    "    print('speak : ')\n",
    "    audio = r.listen(source)\n",
    "    \n",
    "    try:\n",
    "        text = r.recognize_google(audio, language='tr-tr')\n",
    "        print(\"soylediginiz : {}\".format(text))\n",
    "        texts = [text]\n",
    "        tokens = tokenizer.texts_to_sequences(texts)\n",
    "        tokens_pad = pad_sequences(tokens, maxlen= max_tokens)\n",
    "        deger = model.predict(tokens_pad)\n",
    "        print(deger)\n",
    "        if deger > 0.5:\n",
    "            print('Pozitif : \\U0001F642')\n",
    "        else:\n",
    "            print('Negatif : \\U0001F641') \n",
    "        \n",
    "    except:\n",
    "        print(\"anlamadım\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d074a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
